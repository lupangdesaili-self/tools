{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "import torch.nn.functional as F\n",
    "from argparse import Namespace\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "args = Namespace(data_path = '/Users/lijunlin/Documents/研二上/pytorch/data/6000.xlsx',\n",
    "                columns = ['Sentences','Class'],\n",
    "                maxlen = 100,\n",
    "                embedding_dim = 2000,\n",
    "                numwords = 2000,\n",
    "                batch_size = 64,\n",
    "                epochs = 20,\n",
    "                shuffle = 2000)\n",
    "df = pd.read_excel(args.data_path,columns = args.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_preprocessing:\n",
    "    def __init__(self,corpus):\n",
    "        self.sentences = list(corpus)\n",
    "    def sentences_de_comma(self,inputs):\n",
    "        pattern = re.compile(r'[^\\w\\s]')\n",
    "        return [pattern.sub('',str(x)) for x in inputs]\n",
    "    def sentences_de_english(self,inputs):\n",
    "        pattern = re.compile(r'[a-zA-Z]')\n",
    "        return [pattern.sub('',str(x)) for x in inputs]\n",
    "    def washing(self):\n",
    "        inputs = self.sentences\n",
    "        outputs = self.sentences_de_comma(inputs)\n",
    "        outputs = self.sentences_de_english(outputs)\n",
    "        return outputs\n",
    "corpus = word_preprocessing(df['Sentences']).washing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVectorizer:\n",
    "    def __init__(self,corpus,numwords):\n",
    "        self.corpus = corpus\n",
    "        self.model = Tokenizer(num_words = args.numwords)\n",
    "        self.model.fit_on_texts(corpus)\n",
    "    def vectorize(self):\n",
    "        self.sequences = self.model.texts_to_sequences(corpus)\n",
    "        return self.sequences\n",
    "    def padding(self,maxlen):\n",
    "        return pad_sequences(self.vectorize(), maxlen, padding='post')\n",
    "    def get_index_word(self):\n",
    "        return self.model.index_word\n",
    "    def get_word_index(self):\n",
    "        index_2_word_dic = self.model.index_word\n",
    "        word_2_index_dic = {x:y for y,x in index_2_word_dic.items()}\n",
    "        return word_2_index_dic\n",
    "vectorizer = MyVectorizer(corpus,args.numwords)\n",
    "X = vectorizer.padding(args.maxlen)\n",
    "index_2_word = vectorizer.get_index_word()\n",
    "word_2_index = vectorizer.get_word_index()\n",
    "y = np.asarray(df.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class X_y_preprocessing:\n",
    "    def __init__(self,X,y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.X_train,self.X_test,self.y_train,self.y_test = train_test_split(self.X,self.y,test_size= 0.3)\n",
    "    def get_db_train(self):\n",
    "        db_train = TensorDataset(self.X_train,self.y_train)\n",
    "        db_train = DataLoader(db_train,batch_size = args.batch_size,shuffle = True, drop_last= True)\n",
    "        return db_train\n",
    "    def get_db_test(self):\n",
    "        db_test = TensorDataset(self.X_test,self.y_test)\n",
    "        db_test = DataLoader(db_test, batch_size = args.batch_size, shuffle = True, drop_last=True)\n",
    "        return db_test\n",
    "loader = X_y_preprocessing(X,y)\n",
    "db_train = loader.get_db_train()\n",
    "db_test = loader.get_db_test()\n",
    "X_train = loader.X_train\n",
    "y_train = loader.y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,h_dim):\n",
    "        super().__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.projection = nn.Sequential(nn.Linear(h_dim,64),\n",
    "                                       nn.ReLU(True),\n",
    "                                       nn.Linear(64,1))\n",
    "    def forward(self,x):\n",
    "        energy = self.projection(x)  ###[b,ml,h_dim（*2）] -> [b,ml,1] (消掉隐藏层维度)   \n",
    "        attn_score = F.softmax(energy.squeeze(-1),dim = 1).unsqueeze(-1)   ##[b,ml,1]->[b,ml]->[b,ml,1]    \n",
    "        outputs = torch.bmm(x.transpose(1,2),attn_score).squeeze()   ##[b,ml,h_dim（*2）]*[b,ml,1]   \n",
    "        #outputs_2 = (x*attn_score).sum(dim=1)   #[b,ml,h_dim]*[b,ml,1]\n",
    "        return outputs,attn_score\n",
    "class Attention_LSTM(nn.Module):\n",
    "    def __init__(self,numwords,batch_size,embedding_dim,hidden_size,bidirectional,output_dim,num_layers):\n",
    "        super(Attention_LSTM,self).__init__()\n",
    "        self.running_loss = 0\n",
    "        self.direction_num = int(bidirectional)+1\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(num_embeddings = numwords,embedding_dim = embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,\n",
    "                           hidden_size = hidden_size,\n",
    "                           num_layers = num_layers,\n",
    "                           batch_first = True,\n",
    "                           bidirectional = bidirectional,\n",
    "                           dropout = 0.1)\n",
    "        self.attention = Attention(h_dim = hidden_size)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_size,output_dim),\n",
    "                                nn.Softmax(dim = 1))\n",
    "    def forward(self,x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_output,h_state = self.lstm(embedded)\n",
    "        lstm_output = lstm_output[:,:,:hidden_size]+lstm_output[:,:,hidden_size:]\n",
    "        attn_output,attn_weights = self.attention(lstm_output)\n",
    "        output = self.fc(attn_output.view(batch_size, -1))\n",
    "        return output,attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_batches(db):\n",
    "    y = len(list(enumerate(db)))\n",
    "    return y\n",
    "def get_accruacy(out,y):\n",
    "    out_class = out.max(1)[1].numpy()\n",
    "    acc_num = np.sum(np.array(np.equal(out_class,y)))\n",
    "    acc = acc_num/len(y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc= 0\n",
    "    for batch in enumerate(data_loader):\n",
    "        i,(X,y) = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(X)\n",
    "        loss = criterion(outputs,y)\n",
    "        acc = get_accruacy(outputs,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += (loss.item()-epoch_loss)/(i+1)\n",
    "        epoch_acc  += (acc-epoch_acc)/(i+1)\n",
    "        train_bar.update()\n",
    "        train_bar.set_postfix(loss = epoch_loss,acc = epoch_acc)\n",
    "    train_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "    epoch_bar.set_postfix(loss = epoch_loss,\n",
    "                         acc = epoch_acc)\n",
    "def evaluate(data_loader,model,criterion):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    for batch in enumerate(data_loader):\n",
    "        i,(X,y) = batch\n",
    "        outputs, _ = model(X)\n",
    "        loss = criterion(outputs,y)\n",
    "        acc = get_accruacy(outputs,y)\n",
    "        eval_loss += (loss.item()-eval_loss)/(i+1)\n",
    "        eval_acc += (acc-eval_acc)/(i+1)\n",
    "        eval_bar.set_postfix(loss = eval_loss, acc = eval_acc)\n",
    "        eval_bar.update()\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "max_len = 100\n",
    "output_dim = 3\n",
    "bidirectional = True\n",
    "embedding_dim = args.embedding_dim\n",
    "numwords = args.numwords\n",
    "batch_size = args.batch_size\n",
    "num_layers = 1\n",
    "epochs = args.epochs\n",
    "model = Attention_LSTM(numwords=numwords,\n",
    "                       batch_size=batch_size,\n",
    "                       embedding_dim=embedding_dim,\n",
    "                       hidden_size=hidden_size,bidirectional=bidirectional,output_dim=output_dim,num_layers=num_layers)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fb6c3d9e1343f789b91644aa75d52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=65.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d83d424acf1473f892f8bdcba80e31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', max=20.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4a5d6c53204b44ae90df68a09d9cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                         total=get_num_batches(db_train),\n",
    "                         position=1,\n",
    "                         leave=True)\n",
    "epoch_bar = tqdm_notebook(desc='training routine',\n",
    "                          total=epochs,\n",
    "                          position=0)\n",
    "eval_bar = tqdm_notebook(desc=\"split=evaluate\",\n",
    "                        total=get_num_batches(db_test),\n",
    "                        position=2,\n",
    "                        leave= True)\n",
    "for epoch in range(epochs):\n",
    "    train(db_train,model,optimizer,criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(db_test,model,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weight=np.array(model.embedding.weight.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
